{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc28bddc-f837-4b19-8402-b1fddcbda6a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/your/tiktok_transcripts.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Read files\u001b[39;00m\n\u001b[1;32m     74\u001b[0m youtube_transcript \u001b[38;5;241m=\u001b[39m read_file(youtube_transcript_path)\n\u001b[0;32m---> 75\u001b[0m tiktok_transcripts \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktok_transcripts_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Find TikTok segments\u001b[39;00m\n\u001b[1;32m     78\u001b[0m tiktok_positions \u001b[38;5;241m=\u001b[39m find_tiktok_segments(youtube_transcript, tiktok_transcripts)\n",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(file_path):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/tiktok_transcripts.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def find_tiktok_segments(youtube_transcript, tiktok_transcripts, threshold=0.65):\n",
    "    tiktok_positions = []\n",
    "    youtube_sentences = sent_tokenize(youtube_transcript)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    for tiktok in tiktok_transcripts:\n",
    "        tiktok_sentences = sent_tokenize(tiktok)\n",
    "        \n",
    "        for i in range(len(youtube_sentences) - len(tiktok_sentences) + 1):\n",
    "            segment = youtube_sentences[i:i + len(tiktok_sentences)]\n",
    "            segment_str = ' '.join(segment)\n",
    "            \n",
    "            tfidf_matrix = vectorizer.fit_transform([segment_str, tiktok])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            \n",
    "            if similarity >= threshold:\n",
    "                start_pos = sum(len(s) for s in youtube_sentences[:i]) + i\n",
    "                end_pos = start_pos + len(tiktok)\n",
    "                tiktok_positions.append((start_pos, end_pos))\n",
    "                break\n",
    "                \n",
    "    return tiktok_positions\n",
    "\n",
    "def segment_and_label_transcript(youtube_transcript, tiktok_positions):\n",
    "    tiktok_positions.sort()\n",
    "    segments = []\n",
    "    labels = []\n",
    "    last_end = 0\n",
    "    \n",
    "    for start, end in tiktok_positions:\n",
    "        if start > last_end:\n",
    "            segments.append(youtube_transcript[last_end:start])\n",
    "            labels.append(\"Non Tiktok - 0\")\n",
    "        \n",
    "        segments.append(youtube_transcript[start:end])\n",
    "        labels.append(\"Tiktok - 1\")\n",
    "        \n",
    "        last_end = end\n",
    "    \n",
    "    if last_end < len(youtube_transcript):\n",
    "        segments.append(youtube_transcript[last_end:])\n",
    "        labels.append(\"Non Tiktok - 0\")\n",
    "    \n",
    "    labeled_segments = [f\"{label}:\\n{segment}\\n\" for label, segment in zip(labels, segments)]\n",
    "    \n",
    "    return labeled_segments\n",
    "\n",
    "# File paths\n",
    "youtube_transcript_path = \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#51/#51 Would you lie for fame_55716-10-08_Powered by notta.ai.txt\"\n",
    "tiktok_paths = [\n",
    "    \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#51/yt_51_1_55725-06-17_Powered by notta.ai.txt\",\n",
    "    \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#51/yt_51_2_55725-06-17_Powered by notta.ai.txt\",\n",
    "    \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#51/yt_51_3_55725-06-17_Powered by notta.ai.txt\",\n",
    "    \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#51/yt_51_4_55725-06-17_Powered by notta.ai.txt\"\n",
    "]\n",
    "\n",
    "# Read files\n",
    "youtube_transcript = read_file(youtube_transcript_path)\n",
    "tiktok_transcripts = read_file(tiktok_transcripts_path).split('\\n')\n",
    "\n",
    "# Find TikTok segments\n",
    "tiktok_positions = find_tiktok_segments(youtube_transcript, tiktok_transcripts)\n",
    "\n",
    "# Segment and label the YouTube transcript\n",
    "labeled_segments = segment_and_label_transcript(youtube_transcript, tiktok_positions)\n",
    "\n",
    "# Write to file\n",
    "output_path = os.path.splitext(youtube_transcript_path)[0] + \"_labeled.txt\"\n",
    "write_file(output_path, '\\n'.join(labeled_segments))\n",
    "\n",
    "print(f\"Labeled segments have been written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34e828cb-8f4e-4531-bfe5-4e8e0413d2aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform([last_sentence_tiktok] \u001b[38;5;241m+\u001b[39m segment_sentences)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Calculate Cosine Similarity for sentences\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m sentence_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Find the most similar ending sentence\u001b[39;00m\n\u001b[1;32m     52\u001b[0m most_similar_sentence_index \u001b[38;5;241m=\u001b[39m sentence_similarities\u001b[38;5;241m.\u001b[39margmax()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1578\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \n\u001b[1;32m   1545\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;124;03m    Returns the cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1578\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:173\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    166\u001b[0m         X,\n\u001b[1;32m    167\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:967\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    970\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    974\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to read file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "# Paths to your TikTok and YouTube transcripts\n",
    "tiktok_path = \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#24/yt_24_2_55725-05-31_Powered by notta.ai.txt\"\n",
    "youtube_path = \"/Users/rossjackson/Documents/VideoAnalysisProject/New_Project_Directory/TikTok:YT_Audios/#24/#24 Trust ｜ Cheating Partners, Death, Knife crime and being let down [ZNuYTm4G6RM]_55716-10-19_Powered by notta.ai.txt\"\n",
    "\n",
    "# Read the transcripts\n",
    "tiktok_transcript = read_file(tiktok_path)\n",
    "youtube_transcript = read_file(youtube_path)\n",
    "\n",
    "# Get the length of the TikTok transcript in terms of words\n",
    "tiktok_length = len(tiktok_transcript.split())\n",
    "\n",
    "# Split the YouTube transcript into individual words\n",
    "youtube_words = youtube_transcript.split()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# ... (previous code remains the same)\n",
    "\n",
    "# Get the last sentence of the TikTok transcript\n",
    "last_sentence_tiktok = sent_tokenize(tiktok_transcript)[-1]\n",
    "\n",
    "# Initialize variables to keep track of the most similar ending sentence\n",
    "max_sentence_similarity = 0\n",
    "most_similar_sentence = \"\"\n",
    "\n",
    "# Find the starting index of the most similar segment in the YouTube transcript\n",
    "if most_similar_segment:\n",
    "    start_index = youtube_transcript.find(most_similar_segment)\n",
    "\n",
    "    # Take a larger portion of the YouTube transcript starting from this index\n",
    "    extended_segment = youtube_transcript[start_index:start_index + len(most_similar_segment) * 2]\n",
    "\n",
    "    # Tokenize the extended segment into sentences\n",
    "    segment_sentences = sent_tokenize(extended_segment)\n",
    "\n",
    "    # Vectorize the last sentence of the TikTok transcript and the sentences in the extended segment\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([last_sentence_tiktok] + segment_sentences)\n",
    "\n",
    "    # Calculate Cosine Similarity for sentences\n",
    "    sentence_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "    # Find the most similar ending sentence\n",
    "    most_similar_sentence_index = sentence_similarities.argmax()\n",
    "    most_similar_sentence = segment_sentences[most_similar_sentence_index]\n",
    "\n",
    "    # Create the final segment starting from the first word and ending at the most similar sentence\n",
    "    final_segment_start = most_similar_segment.find(first_word_tiktok)\n",
    "    final_segment_end = extended_segment.find(most_similar_sentence) + len(most_similar_sentence)\n",
    "    final_segment = extended_segment[final_segment_start:final_segment_end]\n",
    "\n",
    "    print(\"Final matched segment from YouTube transcript:\", final_segment)\n",
    "else:\n",
    "    print(\"No segment found with similarity above the threshold.\")\n",
    "\n",
    "\n",
    "# ... (previous code remains the same)\n",
    "\n",
    "if most_similar_segment:\n",
    "    print(f\"Most similar segment: {most_similar_segment}\")  # Debugging line\n",
    "    start_index = youtube_transcript.find(most_similar_segment)\n",
    "    print(f\"Start index: {start_index}\")  # Debugging line\n",
    "    \n",
    "    if start_index != -1:\n",
    "        extended_segment = youtube_transcript[start_index:start_index + len(most_similar_segment) * 2]\n",
    "        print(f\"Extended segment: {extended_segment}\")  # Debugging line\n",
    "\n",
    "        # ... (rest of the code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "782ed30c-20d2-4168-b249-1dbd4b211746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar segment: Could you stay with someone if they cheated on you? The pure convenience. I just can't be asked what's breaking up. But then I'd be like, then you would, then it would be like spring me a sandwich with the one cent of me thing of ketchup. And I'd be like, hey! Yeah, they're always in your debt now. They're always in your debt. Everything, everything. What's the, what's the good thing? You be nitpicking. They'll bring you right about. I want it brown sauce! I want it brown sauce! You did it, you did it. Are you telling me I'm lying? Are you telling me I'm lying? Yeah. Yeah, remember your antics from a year ago. Remember! Well, 15 years ago, that time you sent me that man! Now I'm getting ketchup on my brown sauce! You're a filthy whore! You're a filthy whore! Ah! Ah! Oh! Oh God, I can't say that. No, I fucking, no. If you cheated on me, that's what, that's, that'd be a name! Yeah, yeah. I'd be happy to have been like, I'd be coming. You never forget it. No, of course you wouldn't. You come home even in the good times. How's dinner? It was beautiful, you filthy whore. Yeah. Even my children go and ask your filthy whore for mother. If dinner's ready.\n",
      "Start index: -1\n"
     ]
    }
   ],
   "source": [
    "if most_similar_segment:\n",
    "    print(f\"Most similar segment: {most_similar_segment}\")  # Debugging line\n",
    "    start_index = youtube_transcript.find(most_similar_segment)\n",
    "    print(f\"Start index: {start_index}\")  # Debugging line\n",
    "    \n",
    "    if start_index != -1:\n",
    "        extended_segment = youtube_transcript[start_index:start_index + len(most_similar_segment) * 2]\n",
    "        print(f\"Extended segment: {extended_segment}\")  # Debugging line\n",
    "\n",
    "        # ... (rest of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4af0b060-cf9e-4b04-9400-be469fa219d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start index after trimming: 34297\n"
     ]
    }
   ],
   "source": [
    "# Trim extra whitespaces\n",
    "most_similar_segment = \" \".join(most_similar_segment.split())\n",
    "youtube_transcript = \" \".join(youtube_transcript.split())\n",
    "\n",
    "# Then try to find the index again\n",
    "start_index = youtube_transcript.find(most_similar_segment)\n",
    "print(f\"Start index after trimming: {start_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a929a1d2-8d40-4dc3-aaf0-44a4c0781e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended segment: Could you stay with someone if they cheated on you? The pure convenience. I just can't be asked what...\n",
      "Final matched segment from YouTube transcript: Could you stay with someone if they cheated on you? The pure convenience. I just can't be asked what's breaking up. But then I'd be like, then you would, then it would be like spring me a sandwich with the one cent of me thing of ketchup. And I'd be like, hey! Yeah, they're always in your debt now. They're always in your debt. Everything, everything. What's the, what's the good thing? You be nitpicking. They'll bring you right about. I want it brown sauce! I want it brown sauce! You did it, you did it. Are you telling me I'm lying? Are you telling me I'm lying? Yeah. Yeah, remember your antics from a year ago. Remember! Well, 15 years ago, that time you sent me that man! Now I'm getting ketchup on my brown sauce! You're a filthy whore! You're a filthy whore! Ah! Ah! Oh! Oh God, I can't say that. No, I fucking, no. If you cheated on me, that's what, that's, that'd be a name! Yeah, yeah. I'd be happy to have been like, I'd be coming. You never forget it. No, of course you wouldn't. You come home even in the good times. How's dinner? It was beautiful, you filthy whore. Yeah. Even my children go and ask your filthy whore for mother. If dinner's ready. If dinner's ready? She's like, yeah, even your happy birthday, filthy whore. No, no, there'd be one of them things where it's just like, oh, yeah, yeah, do you like that, Pete? And I'll be like, yeah, I like that. I know Tommy last year liked it, didn't he? Didn't he? Yeah, wait, wait, wait, wait. We should do some good head. Where'd you like that? Where'd you like that? You weren't doing that, yeah, we're going to do that. Yeah, and when we see Tommy out in public, oh, go on then, there he is. Go on. Jump on it. Yeah. Yeah, you could just be hard to let go. That's what I'm saying, once trust is broken, man. It's finished.\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code remains the same)\n",
    "\n",
    "if start_index != -1:\n",
    "    extended_segment = youtube_transcript[start_index:start_index + len(most_similar_segment) * 2]\n",
    "    print(f\"Extended segment: {extended_segment[:100]}...\")  # Debugging line, showing first 100 characters\n",
    "\n",
    "    # Tokenize the extended segment into sentences\n",
    "    segment_sentences = sent_tokenize(extended_segment)\n",
    "\n",
    "    # Vectorize the last sentence of the TikTok transcript and the sentences in the extended segment\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([last_sentence_tiktok] + segment_sentences)\n",
    "\n",
    "    # Calculate Cosine Similarity for sentences\n",
    "    sentence_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "\n",
    "    # Find the most similar ending sentence\n",
    "    most_similar_sentence_index = sentence_similarities.argmax()\n",
    "    most_similar_sentence = segment_sentences[most_similar_sentence_index]\n",
    "\n",
    "    # Create the final segment starting from the first word and ending at the most similar sentence\n",
    "    final_segment_start = most_similar_segment.find(first_word_tiktok)\n",
    "    final_segment_end = extended_segment.find(most_similar_sentence) + len(most_similar_sentence)\n",
    "    final_segment = extended_segment[final_segment_start:final_segment_end]\n",
    "\n",
    "    print(\"Final matched segment from YouTube transcript:\", final_segment)\n",
    "else:\n",
    "    print(\"No segment found with similarity above the threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7840521-9af4-423b-a039-3d5dd1a9128e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
